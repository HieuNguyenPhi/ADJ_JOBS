{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HieuNguyenPhi/ADJ_JOBS/blob/main/notebooks/ADJUST_JOB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# UAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycUR2r1dq1Nm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "\n",
        "account_name = os.getenv('ACCOUNT_NAME')\n",
        "account_key = os.getenv('ACCOUNT_KEY')\n",
        "# Replace with your Azure Storage account name and SAS token or connection string\n",
        "connect_str = f\"DefaultEndpointsProtocol=https;AccountName={account_name};AccountKey={account_key};EndpointSuffix=core.windows.net\"\n",
        "blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
        "container_list = blob_service_client.list_containers()\n",
        "container_name = \"adjuststbuatprocessed\" #os.getenv('CONTAINER_NAME')\n",
        "container_client = blob_service_client.get_container_client(container_name)\n",
        "already_processed = [file.name.split('/')[1].split('.')[0] for file in container_client.list_blobs() if file.name.split('/')[0] == 'output']\n",
        "already_processed[-5:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvcDO8k7s0Jz"
      },
      "outputs": [],
      "source": [
        "from datetime import date\n",
        "import pandas as pd\n",
        "today = date.today().strftime('%Y-%m-%d')\n",
        "need_process = pd.date_range(start=already_processed[-1], end=today).strftime('%Y-%m-%d').to_list()\n",
        "need_process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyPuVC12s7fn"
      },
      "outputs": [],
      "source": [
        "container_name_uat = \"adjuststbuat\"\n",
        "container_client_uat = blob_service_client.get_container_client(container_name_uat)\n",
        "from collections import defaultdict\n",
        "files = [i.name for i in container_client_uat.list_blobs()]\n",
        "groups = defaultdict(list)\n",
        "for f in files:\n",
        "    dt = f.split('_')[1]\n",
        "    groups[dt].append(f)\n",
        "groups[dt]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import polars as pl \n",
        "from tqdm import tqdm\n",
        "storage_options = {\n",
        "    \"account_name\": account_name,\n",
        "    \"account_key\":  account_key,\n",
        "}\n",
        "\n",
        "for ts, files in tqdm(groups.items()):\n",
        "    dt = ts[:10]\n",
        "    if dt not in need_process:\n",
        "        continue\n",
        "    df = pl.scan_csv(f\"az://adjuststbuat/*_{ts}_*.csv.gz\", storage_options = storage_options,glob=True, has_header = True, null_values = [\"\",\"NULL\"], ignore_errors=True).select(pl.all().cast(pl.Utf8))\n",
        "    df.sink_parquet(f\"az://adjuststbuatprocessed/processing/dt={dt}/{ts}.parquet\", storage_options = storage_options, compression=\"snappy\")\n",
        "    print(f'Done dt={dt}/{ts}.parquet')\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for dt in need_process:\n",
        "  df = pl.scan_parquet(f\"az://adjuststbuatprocessed/processing/dt={dt}/*.parquet\", storage_options=storage_options,glob=True).with_columns(pl.lit(dt).alias(\"dt\"))\n",
        "  df.sink_parquet(f\"az://adjuststbuatprocessed/output/{dt}.parquet\", storage_options=storage_options, compression=\"snappy\")\n",
        "  print(f'\\n Done {dt}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TvKr29qyJd_"
      },
      "source": [
        "# Live"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnaFsdZFxpwc"
      },
      "outputs": [],
      "source": [
        "already_processed = [file.name.split('/')[-1].split('.')[0] for file in container_client.list_blobs() if file.name[:12] == 'live/output/']\n",
        "already_processed[-5:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHveM3L5yNix"
      },
      "outputs": [],
      "source": [
        "need_process = pd.date_range(start=already_processed[-1], end=today).strftime('%Y-%m-%d').to_list()\n",
        "need_process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7Er6YEUyOsL"
      },
      "outputs": [],
      "source": [
        "container_name_uat = \"adjuststblive\"\n",
        "container_client_uat = blob_service_client.get_container_client(container_name_uat)\n",
        "from collections import defaultdict\n",
        "files = [i.name for i in container_client_uat.list_blobs()]\n",
        "groups = defaultdict(list)\n",
        "for f in files:\n",
        "    dt = f.split('_')[1]\n",
        "    groups[dt].append(f)\n",
        "groups[dt]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-W3s2nRyRY7"
      },
      "outputs": [],
      "source": [
        "storage_options = {\n",
        "    \"account_name\": account_name,\n",
        "    \"account_key\":  account_key,\n",
        "}\n",
        "\n",
        "for ts, files in tqdm(groups.items()):\n",
        "    dt = ts[:10]\n",
        "    if dt not in need_process:\n",
        "        continue\n",
        "    df = pl.scan_csv(f\"az://adjuststblive/*_{ts}_*.csv.gz\", storage_options = storage_options,glob=True, has_header = True, null_values = [\"\",\"NULL\"], ignore_errors=True).select(pl.all().cast(pl.Utf8))\n",
        "    df.sink_parquet(f\"az://adjuststbuatprocessed/live/processing/dt={dt}/{ts}.parquet\", storage_options = storage_options, compression=\"snappy\")\n",
        "    print(f'Done dt={dt}/{ts}.parquet')\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBm7rfFUyePJ"
      },
      "outputs": [],
      "source": [
        "for dt in need_process:\n",
        "  df = pl.scan_parquet(f\"az://adjuststbuatprocessed/live/processing/dt={dt}/*.parquet\", storage_options=storage_options,glob=True).with_columns(pl.lit(dt).alias(\"dt\"))\n",
        "  df.sink_parquet(f\"az://adjuststbuatprocessed/live/output/{dt}.parquet\", storage_options=storage_options, compression=\"snappy\")\n",
        "  print(f'\\n Done {dt}\\n')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOfzO6uvqUX+Tt89Y73G9E8",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
