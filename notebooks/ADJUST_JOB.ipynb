{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfIkZfhWpeIh1KX/POTUeH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HieuNguyenPhi/ADJ_JOBS/blob/main/notebooks/ADJUST_JOB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycUR2r1dq1Nm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "\n",
        "account_name = os.getenv('ACCOUNT_NAME')\n",
        "account_key = os.getenv('ACCOUNT_KEY')\n",
        "# Replace with your Azure Storage account name and SAS token or connection string\n",
        "connect_str = f\"DefaultEndpointsProtocol=https;AccountName={account_name};AccountKey={account_key};EndpointSuffix=core.windows.net\"\n",
        "blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
        "container_list = blob_service_client.list_containers()\n",
        "container_name = \"adjuststbuatprocessed\" #os.getenv('CONTAINER_NAME')\n",
        "container_client = blob_service_client.get_container_client(container_name)\n",
        "already_processed = [file.name.split('/')[1].split('.')[0] for file in container_client.list_blobs() if file.name.split('/')[0] == 'output']\n",
        "already_processed[-5:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import date\n",
        "import pandas as pd\n",
        "today = date.today().strftime('%Y-%m-%d')\n",
        "need_process = pd.date_range(start=already_processed[-2], end=today).strftime('%Y-%m-%d').to_list()\n",
        "need_process"
      ],
      "metadata": {
        "id": "UvcDO8k7s0Jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "container_name_uat = \"adjuststbuat\"\n",
        "container_client_uat = blob_service_client.get_container_client(container_name_uat)\n",
        "from collections import defaultdict\n",
        "files = [i.name for i in container_client_uat.list_blobs()]\n",
        "groups = defaultdict(list)\n",
        "for f in files:\n",
        "    dt = f.split('_')[1]\n",
        "    groups[dt].append(f)\n",
        "groups[dt]"
      ],
      "metadata": {
        "id": "xyPuVC12s7fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "root = Path.cwd()\n",
        "process_path = f'{root}/process/adjust_uat'\n",
        "from tqdm import tqdm\n",
        "import polars as pl\n",
        "\n",
        "os.makedirs(process_path, exist_ok=True)\n",
        "\n",
        "storage_options = {\n",
        "    \"account_name\": account_name,\n",
        "    \"account_key\":  account_key,\n",
        "}\n",
        "\n",
        "for ts, files in tqdm(groups.items()):\n",
        "    dt = ts[:10]                       # \"2025-06-25\" -> partition dt=...\n",
        "    if dt not in need_process:\n",
        "        continue\n",
        "    # break\n",
        "    partition_dir = os.path.join(process_path, f\"dt={dt}\")\n",
        "    os.makedirs(partition_dir, exist_ok=True)\n",
        "    out_file = os.path.join(partition_dir, f\"{ts}.parquet\")\n",
        "\n",
        "    dfs = []\n",
        "    for f in files:\n",
        "        df = (pl.scan_csv(f\"az://adjuststbuat/{f}\",                             # eager\n",
        "                          storage_options=storage_options,\n",
        "                          has_header=True,\n",
        "                          null_values=[\"\", \"NULL\"])       # rỗng → null\n",
        "                .select(pl.all().cast(pl.Utf8)))          # tất cả cột → string\n",
        "        dfs.append(df)\n",
        "\n",
        "    df_all = pl.concat(dfs, how=\"diagonal\")               # tự thêm null cột thiếu\n",
        "    df_all.sink_parquet(out_file, compression=\"snappy\")\n",
        "    print(f'Done dt={dt}/{ts}.parquet')\n"
      ],
      "metadata": {
        "id": "U34_SUSAs-GQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_folder_path = f\"{root}/process/adjust_uat\"\n",
        "\n",
        "# Replace with the desired folder name in the Azure container\n",
        "azure_folder_name = \"processing\"\n",
        "\n",
        "# Iterate through files in the local folder and upload them\n",
        "for root, dirs, files in tqdm(os.walk(local_folder_path)):\n",
        "    for file in files:\n",
        "        # Construct the full local file path\n",
        "        local_file_path = os.path.join(root, file)\n",
        "\n",
        "        # Construct the blob name (path within the Azure container)\n",
        "        # This preserves the folder structure from the local path\n",
        "        relative_path = os.path.relpath(local_file_path, local_folder_path)\n",
        "        blob_name = os.path.join(azure_folder_name, relative_path)\n",
        "        # print(blob_name)\n",
        "        # Create a blob client for the current file\n",
        "        blob_client = container_client.get_blob_client(blob_name)\n",
        "\n",
        "        print(f\"Uploading {local_file_path} to {container_name}/{blob_name}\")\n",
        "\n",
        "#         # Upload the file\n",
        "        with open(local_file_path, \"rb\") as data:\n",
        "            blob_client.upload_blob(data, overwrite=True)\n",
        "\n",
        "print(\"\\nFolder upload complete.\")"
      ],
      "metadata": {
        "id": "Yl40uY9E7IP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for dt_folder in tqdm(glob.glob(os.path.join(process_path, \"dt=*\"))):\n",
        "  dt = os.path.basename(dt_folder)[3:]\n",
        "  df = pl.scan_parquet(\n",
        "      f\"az://adjuststbuatprocessed/live/processing/dt={dt}/*.parquet\",\n",
        "      storage_options=storage_options,\n",
        "      glob=True,                 # rất quan trọng để expand '*'\n",
        "  ).select(pl.all().cast(pl.Utf8)).with_columns(pl.lit(dt).alias(\"dt\"))\n",
        "  df.sink_parquet(\n",
        "      f\"az://adjuststbuatprocessed/live/output/dt={dt}.parquet\",\n",
        "      storage_options=storage_options,\n",
        "      compression=\"snappy\",\n",
        "  )\n",
        "# output_path = f'{root}/output/adjust_uat'\n",
        "# os.makedirs(output_path, exist_ok=True)\n",
        "# import glob\n",
        "# import shutil\n",
        "# for dt_folder in tqdm(glob.glob(os.path.join(process_path, \"dt=*\"))):\n",
        "#     dt = os.path.basename(dt_folder)[3:]                 # \"2025-06-25\"\n",
        "#     files_pq = glob.glob(os.path.join(dt_folder, \"*T*.parquet\"))\n",
        "#     if not files_pq:\n",
        "#         continue\n",
        "\n",
        "#     out_path = os.path.join(output_path, f\"{dt}.parquet\")\n",
        "\n",
        "#     # Nếu trước đó lỡ tạo cùng tên dưới dạng DIR → xoá\n",
        "#     if os.path.isdir(out_path):\n",
        "#         shutil.rmtree(out_path)\n",
        "\n",
        "#     # ---------- ❶  Lazy scan tất cả Parquet ----------\n",
        "#     lfs = [pl.scan_parquet(f) for f in files_pq]          # mỗi file → LazyFrame\n",
        "\n",
        "#     # ---------- ❷  Concat diagonal + giữ schema linh hoạt ----------\n",
        "#     lf_day = (\n",
        "#         pl.concat(lfs, how=\"diagonal\")                    # tự thêm null cột thiếu\n",
        "#         .select(pl.all().cast(pl.Utf8))                   # đảm bảo mọi cột = string\n",
        "#         .with_columns(pl.lit(dt).alias(\"dt\"))             # thêm cột partition (tuỳ)\n",
        "#     )\n",
        "\n",
        "#     # ---------- ❸  Ghi duy nhất 1 Parquet ----------\n",
        "#     lf_day.sink_parquet(out_path, compression=\"snappy\")"
      ],
      "metadata": {
        "id": "K5p12_Giu1Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eoUxR5sVxMJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# local_folder_path = f\"{root}/output/adjust_uat\"\n",
        "\n",
        "# # Replace with the desired folder name in the Azure container\n",
        "# azure_folder_name = \"output\"\n",
        "\n",
        "# # Iterate through files in the local folder and upload them\n",
        "# for root, dirs, files in tqdm(os.walk(local_folder_path)):\n",
        "#     for file in files:\n",
        "#         # Construct the full local file path\n",
        "#         local_file_path = os.path.join(root, file)\n",
        "\n",
        "#         # Construct the blob name (path within the Azure container)\n",
        "#         # This preserves the folder structure from the local path\n",
        "#         relative_path = os.path.relpath(local_file_path, local_folder_path)\n",
        "#         blob_name = os.path.join(azure_folder_name, relative_path)\n",
        "#         # print(blob_name)\n",
        "#         # Create a blob client for the current file\n",
        "#         blob_client = container_client.get_blob_client(blob_name)\n",
        "\n",
        "#         print(f\"Uploading {local_file_path} to {container_name}/{blob_name}\")\n",
        "\n",
        "# #         # Upload the file\n",
        "#         with open(local_file_path, \"rb\") as data:\n",
        "#             blob_client.upload_blob(data, overwrite=True)\n",
        "\n",
        "# print(\"\\nFolder upload complete.\")"
      ],
      "metadata": {
        "id": "guZytRUpxPdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with the path to the local folder you want to delete\n",
        "local_folder_paths = [f\"{root}/process/adjust_uat\",f\"{root}/output/adjust_uat\"]\n",
        "# local_folder_path = f\"data/process/adjust_live\"\n",
        "for local_folder_path in local_folder_paths:\n",
        "    if os.path.exists(local_folder_path):\n",
        "        print(f\"Deleting local folder: {local_folder_path}\")\n",
        "        shutil.rmtree(local_folder_path)\n",
        "        print(\"Local folder deleted.\")\n",
        "    else:\n",
        "        print(f\"Local folder not found: {local_folder_path}\")"
      ],
      "metadata": {
        "id": "cRg5jAwbxQ7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Live"
      ],
      "metadata": {
        "id": "2TvKr29qyJd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# already_processed = [file.name.split('/')[-1].split('.')[0] for file in container_client.list_blobs() if file.name[:12] == 'live/output/']\n",
        "# already_processed[-5:]"
      ],
      "metadata": {
        "id": "EnaFsdZFxpwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# need_process = pd.date_range(start=already_processed[-1], end=today).strftime('%Y-%m-%d').to_list()\n",
        "# need_process"
      ],
      "metadata": {
        "id": "aHveM3L5yNix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# container_name_uat = \"adjuststblive\"\n",
        "# container_client_uat = blob_service_client.get_container_client(container_name_uat)\n",
        "# from collections import defaultdict\n",
        "# files = [i.name for i in container_client_uat.list_blobs()]\n",
        "# groups = defaultdict(list)\n",
        "# for f in files:\n",
        "#     dt = f.split('_')[1]\n",
        "#     groups[dt].append(f)\n",
        "# groups[dt]"
      ],
      "metadata": {
        "id": "I7Er6YEUyOsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# process_path = f'{root}/process/adjust_live'\n",
        "# os.makedirs(process_path, exist_ok=True)\n",
        "\n",
        "# for ts, files in tqdm(groups.items()):\n",
        "#     # if ts not in need_process:\n",
        "#     #     continue\n",
        "#     dt = ts[:10]                       # \"2025-06-25\" -> partition dt=...\n",
        "#     # print(ts)\n",
        "#     # break\n",
        "#     if dt not in need_process:\n",
        "#         continue\n",
        "#     # break\n",
        "#     partition_dir = os.path.join(process_path, f\"dt={dt}\")\n",
        "#     os.makedirs(partition_dir, exist_ok=True)\n",
        "#     out_file = os.path.join(partition_dir, f\"{ts}.parquet\")\n",
        "\n",
        "#     dfs = []\n",
        "#     for f in files:\n",
        "#         df = (pl.scan_csv(f\"az://adjuststblive/{f}\",                             # eager\n",
        "#                           storage_options=storage_options,\n",
        "#                           has_header=True,\n",
        "#                           null_values=[\"\", \"NULL\"],\n",
        "#                           ignore_errors=True)       # rỗng → null\n",
        "#                 .select(pl.all().cast(pl.Utf8)))          # tất cả cột → string\n",
        "#         dfs.append(df)\n",
        "\n",
        "#     df_all = pl.concat(dfs, how=\"diagonal\")               # tự thêm null cột thiếu\n",
        "#     df_all.sink_parquet(out_file, compression=\"snappy\")\n",
        "#     print(f'Done dt={dt}/{ts}.parquet')"
      ],
      "metadata": {
        "id": "g-W3s2nRyRY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output_path = f'{root}/output/adjust_live'\n",
        "# os.makedirs(output_path, exist_ok=True)\n",
        "# for dt_folder in tqdm(glob.glob(os.path.join(process_path, \"dt=*\"))):\n",
        "#     dt = os.path.basename(dt_folder)[3:]                 # \"2025-06-25\"\n",
        "#     files_pq = glob.glob(os.path.join(dt_folder, \"*T*.parquet\"))\n",
        "#     if not files_pq:\n",
        "#         continue\n",
        "\n",
        "#     out_path = os.path.join(output_path, f\"{dt}.parquet\")\n",
        "\n",
        "#     # Nếu trước đó lỡ tạo cùng tên dưới dạng DIR → xoá\n",
        "#     if os.path.isdir(out_path):\n",
        "#         shutil.rmtree(out_path)\n",
        "\n",
        "#     # ---------- ❶  Lazy scan tất cả Parquet ----------\n",
        "#     lfs = [pl.scan_parquet(f) for f in files_pq]          # mỗi file → LazyFrame\n",
        "\n",
        "#     # ---------- ❷  Concat diagonal + giữ schema linh hoạt ----------\n",
        "#     lf_day = (\n",
        "#         pl.concat(lfs, how=\"diagonal\")                    # tự thêm null cột thiếu\n",
        "#         .select(pl.all().cast(pl.Utf8))                   # đảm bảo mọi cột = string\n",
        "#         .with_columns(pl.lit(dt).alias(\"dt\"))             # thêm cột partition (tuỳ)\n",
        "#     )\n",
        "\n",
        "#     # ---------- ❸  Ghi duy nhất 1 Parquet ----------\n",
        "#     lf_day.sink_parquet(out_path, compression=\"snappy\")"
      ],
      "metadata": {
        "id": "sBm7rfFUyePJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# local_folder_path = f\"{root}/process/adjust_live\"\n",
        "\n",
        "# # Replace with the desired folder name in the Azure container\n",
        "# azure_folder_name = \"live/processing\"\n",
        "\n",
        "# # Iterate through files in the local folder and upload them\n",
        "# for root, dirs, files in tqdm(os.walk(local_folder_path)):\n",
        "#     for file in files:\n",
        "#         # Construct the full local file path\n",
        "#         local_file_path = os.path.join(root, file)\n",
        "\n",
        "#         # Construct the blob name (path within the Azure container)\n",
        "#         # This preserves the folder structure from the local path\n",
        "#         relative_path = os.path.relpath(local_file_path, local_folder_path)\n",
        "#         blob_name = os.path.join(azure_folder_name, relative_path)\n",
        "#         # print(blob_name)\n",
        "#         # Create a blob client for the current file\n",
        "#         blob_client = container_client.get_blob_client(blob_name)\n",
        "\n",
        "#         print(f\"Uploading {local_file_path} to {container_name}/{blob_name}\")\n",
        "\n",
        "# #         # Upload the file\n",
        "#         with open(local_file_path, \"rb\") as data:\n",
        "#             blob_client.upload_blob(data, overwrite=True)\n",
        "\n",
        "# print(\"\\nFolder upload complete.\")"
      ],
      "metadata": {
        "id": "nkJnz3CeykiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# local_folder_path = f\"{root}/output/adjust_live\"\n",
        "\n",
        "# # Replace with the desired folder name in the Azure container\n",
        "# azure_folder_name = \"live/output\"\n",
        "\n",
        "# # Iterate through files in the local folder and upload them\n",
        "# for root, dirs, files in tqdm(os.walk(local_folder_path)):\n",
        "#     for file in files:\n",
        "#         # Construct the full local file path\n",
        "#         local_file_path = os.path.join(root, file)\n",
        "\n",
        "#         # Construct the blob name (path within the Azure container)\n",
        "#         # This preserves the folder structure from the local path\n",
        "#         relative_path = os.path.relpath(local_file_path, local_folder_path)\n",
        "#         blob_name = os.path.join(azure_folder_name, relative_path)\n",
        "#         # print(blob_name)\n",
        "#         # Create a blob client for the current file\n",
        "#         blob_client = container_client.get_blob_client(blob_name)\n",
        "\n",
        "#         print(f\"Uploading {local_file_path} to {container_name}/{blob_name}\")\n",
        "\n",
        "# #         # Upload the file\n",
        "#         with open(local_file_path, \"rb\") as data:\n",
        "#             blob_client.upload_blob(data, overwrite=True)\n",
        "\n",
        "# print(\"\\nFolder upload complete.\")"
      ],
      "metadata": {
        "id": "uJFbJAwtysuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Replace with the path to the local folder you want to delete\n",
        "# local_folder_paths = [f\"{root}/process/adjust_live\",f\"{root}/output/adjust_live\"]\n",
        "# # local_folder_path = f\"data/process/adjust_live\"\n",
        "# for local_folder_path in local_folder_paths:\n",
        "#     if os.path.exists(local_folder_path):\n",
        "#         print(f\"Deleting local folder: {local_folder_path}\")\n",
        "#         shutil.rmtree(local_folder_path)\n",
        "#         print(\"Local folder deleted.\")\n",
        "#     else:\n",
        "#         print(f\"Local folder not found: {local_folder_path}\")"
      ],
      "metadata": {
        "id": "dtaKWhzhyx3j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}